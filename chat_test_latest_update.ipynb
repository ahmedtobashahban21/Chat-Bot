{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301c0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel , AutoTokenizer , AutoModelForCausalLM\n",
    "import torch\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"E:\\\\model\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"E:\\\\model\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "## utility functions ##\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def to_data(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cpu()\n",
    "    return x.data.numpy()\n",
    "\n",
    "def to_var(x):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.Tensor(x)\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return x\n",
    "\n",
    "def display_dialog_history(dialog_hx):\n",
    "    for j, line in enumerate(dialog_hx):\n",
    "        msg = tokenizer.decode(line)\n",
    "        if j %2 == 0:\n",
    "            print(\">> User: \"+ msg)\n",
    "        else:\n",
    "            print(\"Bot: \"+msg)\n",
    "            print()\n",
    "\n",
    "def generate_next(bot_input_ids, do_sample=True, top_k=10, top_p=.92,\n",
    "                  max_length=1000, pad_token=tokenizer.eos_token_id):\n",
    "    full_msg = model.generate(bot_input_ids, do_sample=True,\n",
    "                                              top_k=top_k, top_p=top_p,\n",
    "                                              max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "    msg = to_data(full_msg.detach()[0])[bot_input_ids.shape[-1]:]\n",
    "    return msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de878c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = [\n",
    "    \"Hello! Escula is a communication platform designed to help parents, students, and schools stay connected and informed about everything related to education.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c64191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get personality facts for conversation\n",
    "personas = [pers+tokenizer.eos_token for pers in persona]\n",
    "\n",
    "personas = tokenizer.encode(''.join(['<|p2|>'] + personas + ['<|sep|>'] + ['<|start|>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf199afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "# Function to handle user input and display bot's response\n",
    "def handle_input():\n",
    "    user_input = user_input_entry.get()\n",
    "    user_input_entry.delete(0, tk.END)\n",
    "\n",
    "    # Encode the user input\n",
    "    user_inp = tokenizer.encode(user_input + tokenizer.eos_token)\n",
    "    # Append to the chat history\n",
    "    dialog_hx.append(user_inp)\n",
    "\n",
    "    # Generate a response while limiting the total chat history to 1000 tokens\n",
    "    bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\n",
    "    msg = generate_next(bot_input_ids)\n",
    "    dialog_hx.append(msg)\n",
    "    bot_response = tokenizer.decode(msg, skip_special_tokens=True)\n",
    "\n",
    "    # Display bot's response\n",
    "    chat_log.insert(tk.END, \"User: {}\\n\".format(user_input))\n",
    "    chat_log.insert(tk.END, \"Bot: {}\\n\".format(bot_response))\n",
    "    chat_log.insert(tk.END, \"\\n\")\n",
    "\n",
    "# Create the GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Chatbot\")\n",
    "\n",
    "# Create chat log display\n",
    "chat_log = tk.Text(root, width=50, height=20)\n",
    "chat_log.pack()\n",
    "\n",
    "# Create user input entry field\n",
    "user_input_entry = tk.Entry(root, width=50)\n",
    "user_input_entry.pack()\n",
    "\n",
    "# Create submit button\n",
    "submit_button = tk.Button(root, text=\"Submit\", command=handle_input)\n",
    "submit_button.pack()\n",
    "\n",
    "# Start the conversation\n",
    "dialog_hx = []\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed5962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
